{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff5a67b-e114-483c-a26a-47a563a5093b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3971139f-99c8-46a4-940b-e9279db65da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Complex Transformations\n",
    "\n",
    "Querying tabular data stored in the data intelligence platform with Spark SQL is easy, efficient, and fast.\n",
    "\n",
    "This gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use **`.`** and **`:`** syntax to query nested data\n",
    "- Parse JSON strings into structs\n",
    "- Flatten and unpack arrays and structs\n",
    "- Combine datasets using joins\n",
    "- Reshape data using pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dbbaf7d-837c-4f05-9ecb-4975a10eead8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cceb7f6-0c41-4b3a-9db1-ba7821db9e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b44c1752-9708-4c94-b4b5-ad45e1b049cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6c4b46-ff73-4e98-bc95-cebf65f5f10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Overview\n",
    "\n",
    "The **`events_raw`** table was registered against data representing a Kafka payload. In most cases, Kafka data will be binary-encoded JSON values. \n",
    "\n",
    "Let's cast the **`key`** and **`value`** as strings to view these values in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "560f3278-18ad-427e-acb2-520ca2187696",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Note the two ways to code a CAST"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW events_strings AS \n",
    "SELECT \n",
    "  CAST(key AS string) AS key,  string(value) \n",
    "FROM events_raw;\n",
    "\n",
    "\n",
    "\n",
    "SELECT * \n",
    "FROM events_strings LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dfbcf14-5170-43fc-ab28-9e4239615abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see from the results above, the data consists of a unique key and a JSON string of event data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d438da-7d35-40cd-a4c1-0fbc34c94695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulate Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03f8368-1401-4269-8659-9bdf7daf0054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Work with Nested Data\n",
    "Spark SQL has built-in functionality to directly interact with nested data stored as JSON strings or struct types.\n",
    "- Use **`:`** syntax in queries to access subfields in JSON strings\n",
    "- Use **`.`** syntax in queries to access subfields in struct types\n",
    "\n",
    "Let's step into the **`value`** column and grab one row of data with an **`event_name`** of \"finalize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3ce89c-a9dc-4e70-99ee-8b58fba0428d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM events_strings \n",
    "WHERE value:event_name = \"finalize\" \n",
    "ORDER BY key \n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bcf481-b307-44ea-9a3d-2023e2139d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's use the **JSON** string example above to derive the schema, then parse the entire **JSON** column into **STRUCT** types.\n",
    "- **`schema_of_json()`** returns the schema derived from an example **JSON** string.\n",
    "- **`from_json()`** parses a column containing a **JSON** string into a **STRUCT** type using the specified schema.\n",
    "\n",
    "After we unpack the **JSON** string to a **STRUCT** type, let's unpack and flatten all **STRUCT** fields into columns.\n",
    "\n",
    "**`*`** unpacking can be used to flatten a **STRUCT**; **`col_name.*`** pulls out the subfields of **`col_name`** into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e088457-2dd7-4581-bbe4-59ec50267e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}') AS schema;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060eb2f0-e5cf-4dce-a82c-e5372bdc5582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_events AS \n",
    "SELECT json.* \n",
    "FROM (\n",
    "  SELECT from_json(value, 'STRUCT<device: STRING, ecommerce: STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name: STRING, event_previous_timestamp: BIGINT, event_timestamp: BIGINT, geo: STRUCT<city: STRING, state: STRING>, items: ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source: STRING, user_first_touch_timestamp: BIGINT, user_id: STRING>') AS json \n",
    "  FROM events_strings);\n",
    "\n",
    "\n",
    "SELECT * \n",
    "FROM parsed_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa64d9f-c95d-4251-abbc-ea3b3d6a1187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulate Arrays\n",
    "\n",
    "Spark SQL has a number of functions for manipulating array data, including the following:\n",
    "- **`explode()`** separates the elements of an array into multiple rows; this creates a new row for each element.\n",
    "- **`size()`** provides a count for the number of elements in an array for each row.\n",
    "\n",
    "The code below explodes the **`items`** field (an array of structs) into multiple rows and shows events containing arrays with 3 or more items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5ed672-b4ae-4a84-ad2e-0ee1ad52f521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW exploded_events AS\n",
    "SELECT  *, \n",
    "  explode(items) AS item\n",
    "FROM parsed_events;\n",
    "\n",
    "-- Find users with more than 2 items in their cart\n",
    "SELECT * \n",
    "FROM exploded_events \n",
    "WHERE size(items) > 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec15a6f1-1787-4430-ad62-c77c3b4a4dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Confirm data type for new exploded 'item' column\n",
    "DESCRIBE exploded_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82824008-4af8-45f3-bca0-be13f962ae07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nesting Functions\n",
    "We may want to see a list of all events associated with each **`user_id`** and we can collect all items that have been in a user's cart at any time for any event. Let's walk through how we can accomplish this.\n",
    "### Step 1\n",
    "We use **`collect_set()`** to gather (\"collect\") all unique values in a group, including arrays. We use it here to collect all unique **`item_id`**'s in our **`items`** array of structs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22c992a-8062-404a-9c05-9f61aabc721d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  user_id,\n",
    "  collect_set(items.item_id) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0dbc193-087e-4925-a740-4d3bfde2c000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this case, our result for the list of **`item_id`**'s is an array nested in another array, so let's use **`flatten()`** to pull all items into a single array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b1d379-21f3-4fea-bde5-795a121ca33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Note we have striped off the outer most [] brackets from above Output by using 'flatten'\n",
    "SELECT \n",
    "  user_id,\n",
    "  flatten(collect_set(items.item_id)) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b512df14-ac59-4769-aff2-3661c6d5527d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Because there were multiple sets of items involved, there are duplicate values in our array. We use **`array-distinct()`** to remove these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d2f7cd-9145-41b7-916c-b37b11189759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Navigate to Row 97. Compare it to Row 97 of previous output. Note the difference.\n",
    "SELECT \n",
    "  user_id,\n",
    "  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faac463c-d98f-4adb-8385-fc036b21ea75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the following cell, we combine **`collect_set()`**, **`flatten()`**, and **`array_distinct()`** to accomplish what we desire:\n",
    "\n",
    "We use **`collect_set`** twice in the cell below: once to collect all **`event_name`**'s, and again on the **`item_id`**'s in the **`item`** column. We nest the second call to **`collect_set`** in our **`flatten()`** and **`array_distinct`** calls as outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dff2bbd-1572-4f1a-92e9-2ecb292ff653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  user_id,\n",
    "  collect_set(event_name) AS event_history,\n",
    "  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d082c98-c4d0-4ce3-aafc-2495b74b4d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combine and Reshape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056770e2-53a0-46bb-a843-c772fd24d47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Tables\n",
    "\n",
    "Spark SQL supports standard **`JOIN`** operations (inner, outer, left, right, anti, cross, semi).  \n",
    "Here we join the exploded events dataset with a lookup table to grab the standard printed item name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2295f8-8a27-4173-a6c7-d9d70944562f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW item_purchases AS\n",
    "SELECT * \n",
    "FROM (SELECT *, explode(items) AS item \n",
    "      FROM sales) a\n",
    "INNER JOIN item_lookup b\n",
    "ON a.item.item_id = b.item_id;\n",
    "\n",
    "\n",
    "SELECT * \n",
    "FROM item_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af99d9e2-c2a6-4b0b-ba1a-36451186949b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot Tables\n",
    "\n",
    "We can use **`PIVOT`** to view data from different perspectives by rotating unique values in a specified pivot column into multiple columns based on an aggregate function.\n",
    "- The **`PIVOT`** clause follows the table name or subquery specified in a **`FROM`** clause, which is the input for the pivot table.\n",
    "- Unique values in the pivot column are grouped and aggregated using the provided aggregate expression, creating a separate column for each unique value in the resulting pivot table.\n",
    "\n",
    "The following code cell uses **`PIVOT`** to flatten out the item purchase information contained in several fields derived from the **`sales`** dataset. This flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9c278a-6720-499e-b709-ee2df0409910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM item_purchases\n",
    "PIVOT (\n",
    "  sum(item.quantity) FOR item_id IN (\n",
    "    'P_FOAM_K',\n",
    "    'M_STAN_Q',\n",
    "    'P_FOAM_S',\n",
    "    'M_PREM_Q',\n",
    "    'M_STAN_F',\n",
    "    'M_STAN_T',\n",
    "    'M_PREM_K',\n",
    "    'M_PREM_F',\n",
    "    'M_STAN_K',\n",
    "    'M_PREM_T',\n",
    "    'P_DOWN_S',\n",
    "    'P_DOWN_K')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f278d2f0-b454-4d80-b961-8f0d141408a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "5 - Complex Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}