{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "689d1238-19d7-4fa5-9cd6-4c63fca78675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde53463-28a6-4f2d-b544-f5b71095ba78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data Lab\n",
    "\n",
    "In this lab, you will load data into new and existing Delta tables.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you should be able to:\n",
    "- Create an empty Delta table with a provided schema\n",
    "- Use `COPY INTO` and `CAST` to ingest data to an existing Delta table\n",
    "- Use a CTAS statement to create a Delta table from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4edebdac-b187-4a50-9c6c-6f314cc65502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3735d607-4225-4e2a-97b0-936846cde2e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d752a6-7fb6-4d98-8822-c901ec42ab6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-3L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d44d3c-5397-44f6-be06-4289f9658a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "We will work with a sample of raw Kafka data written as JSON files. \n",
    "\n",
    "Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file. \n",
    "\n",
    "The schema for the table:\n",
    "\n",
    "| field  | type | description |\n",
    "| ------ | ---- | ----------- |\n",
    "| key    | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
    "| offset | BIGINT | This is a unique value, monotonically increasing for each partition |\n",
    "| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
    "| timestamp | BIGINT    | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |\n",
    "| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
    "| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780746c0-aab7-4ab9-b177-61b2b5ab6e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define Schema for Empty Delta Table\n",
    "Create an empty managed Delta table named **`events_bronze`** using the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08e496fe-b227-4a04-a668-1813866da1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08102e25-aa8a-4d0a-9007-2477d731b4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to confirm the table was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a975a8-2eb8-40ed-923a-082796bdfac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "assert spark.catalog.tableExists(\"events_bronze\"), \"The table does not exist\"\n",
    "assert spark.table(\"events_bronze\").count() == 0, \"The table should have 0 records\"\n",
    "for i in ['key','offset','partition','timestamp','topic','value']:\n",
    "  assert i in spark.table(\"events_bronze\").columns, f\"The column {i} is missing\"\n",
    "assert str(spark.table(\"events_bronze\").schema['key'].dataType) == 'BinaryType()', \"Column key is wrong type\"\n",
    "assert str(spark.table(\"events_bronze\").schema['offset'].dataType) == 'LongType()', \"Column offset is wrong type\"\n",
    "assert str(spark.table(\"events_bronze\").schema['partition'].dataType) == 'IntegerType()', \"Column partition is wrong type\"\n",
    "assert str(spark.table(\"events_bronze\").schema['timestamp'].dataType) == 'LongType()', \"Column timestamp is wrong type\"\n",
    "assert str(spark.table(\"events_bronze\").schema['topic'].dataType) == 'StringType()', \"Column topic is wrong type\"\n",
    "assert str(spark.table(\"events_bronze\").schema['value'].dataType) == 'BinaryType()', \"Column value is wrong type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2bf4b0-59e5-4d1e-acbf-2a51ad561b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using `CAST` with JSON Data\n",
    "In the next cell, you will use COPY INTO to ingest data into the table.  \n",
    "  \n",
    "In order to force the JSON data to fit the schema you used when you created the table, you will need to use `CAST` keyword. The syntax for `CAST` is `CAST(column AS data_type)`.  To use `CAST` with `COPY INTO`, replace the path in the `COPY INTO` command you learned in the previous lesson, with a SELECT query (make sure you include the parentheses):\n",
    "  \n",
    "  <code>(SELECT\n",
    "  CAST(key AS BINARY) AS key,<br />\n",
    "  CAST(offset AS BIGINT) AS offset,<br />\n",
    "  CAST(partition AS INT) AS partition,<br />\n",
    "  CAST(timestamp AS BIGINT) AS timestamp,<br />\n",
    "  CAST(topic AS STRING) AS topic,<br />\n",
    "  CAST(value AS BINARY) AS value<br />\n",
    "FROM '/Volumes/dbacademy_ecommerce/v01/raw/events-kafka/')</code>\n",
    "  \n",
    "Note: Because the data files are in JSON format, you will not need to use the \"delimiter\" or \"header\" options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65848f3d-b5ef-43d8-870e-98c8edc64e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67982947-c740-49dc-bbcf-7e8491357cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Manually review the table contents to ensure data was written as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e817d641-cc04-4262-989f-115fd8c28552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2fd01a-23a6-41f7-84e0-c7ec6be6deb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to confirm the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542f4fe4-3c61-4c59-bb42-af4ceecfafab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "assert spark.catalog.tableExists(\"events_bronze\"), \"The table does not exist\"\n",
    "assert spark.table(\"events_bronze\").count() == 2252, \"The table should have 2252 records\"\n",
    "\n",
    "first_five = [r[\"timestamp\"] for r in spark.table(\"events_bronze\").orderBy(F.col(\"timestamp\").asc()).limit(5).collect()]\n",
    "assert first_five == [1593879303631, 1593879304224, 1593879305465, 1593879305482, 1593879305746], \"First 5 values are not correct\"\n",
    "\n",
    "last_five = [r[\"timestamp\"] for r in spark.table(\"events_bronze\").orderBy(F.col(\"timestamp\").desc()).limit(5).collect()]\n",
    "assert last_five == [1593881096290, 1593881095799, 1593881093452, 1593881093394, 1593881092076], \"Last 5 values are not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68179ff7-e762-4abf-a18d-8e56de97725c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Delta Table From Query Results\n",
    "\n",
    "In addition to new events data, let's also load a small lookup table that provides product details that we'll use later in the course.\n",
    "Use a CTAS statement to create a managed Delta table named **`item_lookup`** that extracts data from the parquet directory provided below. \n",
    "\n",
    "Parquet files directory: `/Volumes/dbacademy_ecommerce/v01/raw/item-lookup/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd4c1c6-4e3a-4118-9ccc-5140d93a74d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a1f006a-a29d-46db-9778-3908ea098cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to confirm the lookup table has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145cda37-2651-4c83-a66f-85c36f289058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM item_lookup \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b592605a-812e-4dc1-9c16-09c122a1ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "assert spark.catalog.tableExists(\"item_lookup\"), \"The table does not exist\"\n",
    "\n",
    "actual_values = [r[\"item_id\"] for r in spark.table(\"item_lookup\").collect()]\n",
    "expected_values = ['M_PREM_Q','M_STAN_F','M_PREM_F','M_PREM_T','M_PREM_K','P_DOWN_S','M_STAN_Q','M_STAN_K','M_STAN_T','P_FOAM_S','P_FOAM_K','P_DOWN_K']\n",
    "assert actual_values == expected_values, \"Does not contain the 12 expected item IDs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e596f851-1dc0-42a1-93e7-a6ebcf0ddf6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3L - Load Data Lab",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}